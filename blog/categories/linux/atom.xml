<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Linux | 赵成辉的博客]]></title>
  <link href="http://feiji1hao.github.io/blog/categories/linux/atom.xml" rel="self"/>
  <link href="http://feiji1hao.github.io/"/>
  <updated>2015-05-14T13:03:58+08:00</updated>
  <id>http://feiji1hao.github.io/</id>
  <author>
    <name><![CDATA[赵成辉]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Kubernetes解析：services]]></title>
    <link href="http://feiji1hao.github.io/blog/2014/11/03/kubernetes-services/"/>
    <updated>2014-11-03T00:00:00+08:00</updated>
    <id>http://feiji1hao.github.io/blog/2014/11/03/kubernetes-services</id>
    <content type="html"><![CDATA[<p>Kubernetes从0.4开始，开发很快，变化也很大。这里谈谈services的一些变化。
之前的kubernetes services，kube-proxy在host上监听service指定的端口，然后由kube-proxy将请求转到后端具体的pods。这有一些问题：</p>

<p>（1）最大的问题，由于kube-proxy在host上监听端口，这样pod内的容器在expose时，就不能使用该host的该端口；而且service之间可能冲突；</p>

<p>（2）环境变量不能动态更新，所以pods不能知道在它之后启动的service的信息。</p>

<p>为此，kubernetes重新设计了services，让每个service都有一个自己的IP，但这并不是一个真正IP，而是通过iptables实现一个虚拟的IP。每个节点都生成这样一条类似的iptables规则：</p>

<pre><code class="sh">-A KUBE-PROXY -d ${service-ip}/32 -p tcp -m comment --comment apache-service -m tcp --dport ${service-port} -j REDIRECT --to-ports ${kube-proxy-port}
</code></pre>

<p>由于kube-proxy-port是随机生成，大大减少了host port冲突的机会。另外，由于每个service都有一个自己的IP，所以service之间冲突也没有了，不同的service可以使用相同的port。</p>

<p>考虑3个节点：</p>

<p>yy1: 172.16.213.138 (master)</p>

<p>yy2: 172.16.213.140 (minion)</p>

<p>yy3: 172.16.213.141 (minion)</p>

<p>apiserver运行参数</p>

<pre><code class="sh">＃kube-apiserver --logtostderr=true --v=0 --etcd_servers=http://172.16.213.138:4001 --address=0.0.0.0 --port=8080 --machines=172.16.213.140,172.16.213.141 --minion_port=10250 --allow_privileged=true --portal_net=10.11.0.0/16
</code></pre>

<p>定义services</p>

<pre><code class="sh">$ cat apache-service.json
{
  "id": "apache-service",
  "kind": "Service",
  "apiVersion": "v1beta1",
  "port": 12345,
  "containerPort": 80,
  "selector": {
    "name": "apache"
  }
}
</code></pre>

<p>port为services的端口，kubernetes会为该service选择一个portal_net子网内的一个IP。
如下：</p>

<pre><code class="sh">$ kubecfg -c apache-service.json create services
ID                  Labels              Selector            IP                  Port
----------          ----------          ----------          ----------          ----------
apache-service                          name=apache         10.11.0.1           12345
</code></pre>

<p>可以看看yy2，yy3上iptables的变化。</p>

<p>yy2</p>

<pre><code class="sh">[yy@yy2 ~]$ sudo iptables -nvL -t nat
Chain KUBE-PROXY (2 references)
 pkts bytes target     prot opt in     out     source               destination         
    1    60 REDIRECT   tcp  --  *      *       0.0.0.0/0            10.11.0.1            /* apache-service */ tcp dpt:12345 redir ports 33945

[yy@yy2 ~]$ sudo netstat -ltnp|grep 33945
tcp6       0      0 :::33945                :::*                    LISTEN      247/kube-proxy
</code></pre>

<p>yy3</p>

<pre><code class="sh">[yy@yy3 ~]$ sudo iptables -nvL -t nat
Chain KUBE-PROXY (2 references)
 pkts bytes target     prot opt in     out     source               destination         
0     0 REDIRECT   tcp  --  *      *       0.0.0.0/0            10.11.0.1            /* apache-service */ tcp dpt:12345 redir ports 47163

[yy@yy3 ~]$ sudo netstat -ltnp|grep 47163
tcp6       0      0 :::47163                :::*                    LISTEN      255/kube-proxy
</code></pre>

<p>整体结构大致如下：
<img src="/assets/2014-11-03-kubernetes-services.png" alt="" /></p>

<p>更多内容请参考
<a href="https://github.com/GoogleCloudPlatform/kubernetes/blob/master/docs/services.md">https://github.com/GoogleCloudPlatform/kubernetes/blob/master/docs/services.md</a></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[从veth看虚拟网络设备的qdisc]]></title>
    <link href="http://feiji1hao.github.io/blog/2014/10/31/veth/"/>
    <updated>2014-10-31T00:00:00+08:00</updated>
    <id>http://feiji1hao.github.io/blog/2014/10/31/veth</id>
    <content type="html"><![CDATA[<h2>背景</h2>

<p>前段时间在测试docker的网络性能的时候，发现了一个veth的性能问题，后来给docker官方提交了一个PR，参考<a href="https://github.com/docker/libcontainer/pull/193">set tx_queuelen to 0 when create veth device</a>，引起了一些讨论。再后来，RedHat的网络专家<a href="https://github.com/netoptimizer">Jesper Brouer</a> 出来详细的讨论了一下这个问题。</p>

<p><img src="http://images.cnitblog.com/blog/23202/201410/141941365298477.png" alt="veth qdisc" /></p>

<p>可以看到，veth设备qdisc队列，而环回设备/桥接设备是没qdisc队列的，参考br_dev_setup函数。</p>

<h2>内核实现</h2>

<p>在注册（创建）设备时，qdisc设置为noop_qdisc，
register_netdevice -> dev_init_scheduler</p>

<pre><code class="c">void dev_init_scheduler(struct net_device *dev)
{
    dev-&gt;qdisc = &amp;noop_qdisc;
    netdev_for_each_tx_queue(dev, dev_init_scheduler_queue, &amp;noop_qdisc);
    dev_init_scheduler_queue(dev, &amp;dev-&gt;rx_queue, &amp;noop_qdisc);

    setup_timer(&amp;dev-&gt;watchdog_timer, dev_watchdog, (unsigned long)dev);
}
</code></pre>

<p>打开设备时，如果没有配置qdisc时，就指定为默认的pfifo_fast队列：
dev_open -> dev_activate，</p>

<pre><code class="c">void dev_activate(struct net_device *dev)
{
    int need_watchdog;

    /* No queueing discipline is attached to device;
       create default one i.e. pfifo_fast for devices,
       which need queueing and noqueue_qdisc for
       virtual interfaces
     */

    if (dev-&gt;qdisc == &amp;noop_qdisc)
        attach_default_qdiscs(dev);
...
}

static void attach_default_qdiscs(struct net_device *dev)
{
    struct netdev_queue *txq;
    struct Qdisc *qdisc;

    txq = netdev_get_tx_queue(dev, 0);

    if (!netif_is_multiqueue(dev) || dev-&gt;tx_queue_len == 0) {
        netdev_for_each_tx_queue(dev, attach_one_default_qdisc, NULL);
        dev-&gt;qdisc = txq-&gt;qdisc_sleeping;
        atomic_inc(&amp;dev-&gt;qdisc-&gt;refcnt);
    } else {///multi queue
        qdisc = qdisc_create_dflt(dev, txq, &amp;mq_qdisc_ops, TC_H_ROOT);
        if (qdisc) {
            qdisc-&gt;ops-&gt;attach(qdisc);
            dev-&gt;qdisc = qdisc;
        }
    }
}

static void attach_one_default_qdisc(struct net_device *dev,
                     struct netdev_queue *dev_queue,
                     void *_unused)
{
    struct Qdisc *qdisc;

    if (dev-&gt;tx_queue_len) {
        qdisc = qdisc_create_dflt(dev, dev_queue,
                      &amp;pfifo_fast_ops, TC_H_ROOT);
        if (!qdisc) {
            printk(KERN_INFO "%s: activation failed\n", dev-&gt;name);
            return;
        }

        /* Can by-pass the queue discipline for default qdisc */
        qdisc-&gt;flags |= TCQ_F_CAN_BYPASS;
    } else {
        qdisc =  &amp;noqueue_qdisc;
    }
    dev_queue-&gt;qdisc_sleeping = qdisc;
}
</code></pre>

<h2>创建noqueue</h2>

<p>开始尝试直接删除设备默认的pfifo_fast队列，发现会出错：</p>

<pre><code class="bash"># tc qdisc del dev vethd4ea root
RTNETLINK answers: No such file or directory
# tc  -s qdisc ls dev vethd4ea
qdisc pfifo_fast 0: root refcnt 2 bands 3 priomap  1 2 2 2 1 2 0 0 1 1 1 1 1 1 1 1
 Sent 29705382 bytes 441562 pkt (dropped 0, overlimits 0 requeues 0) 
 backlog 0b 0p requeues 0 
</code></pre>

<p>后来看到Jesper Brouer给出一个替换默认队列的方式，尝试了一下，成功完成。</p>

<p>替换默认的qdisc队列</p>

<pre><code class="bash"># tc qdisc replace dev vethd4ea root pfifo limit 100
# tc  -s qdisc ls dev vethd4ea                      
qdisc pfifo 8001: root refcnt 2 limit 100p
 Sent 264 bytes 4 pkt (dropped 0, overlimits 0 requeues 0) 
 backlog 0b 0p requeues 0 
# ip link show vethd4ea
9: vethd4ea: &lt;BROADCAST,UP,LOWER_UP&gt; mtu 1500 qdisc pfifo master docker0 state UP mode DEFAULT qlen 1000
link/ether 3a:15:3b:e1:d7:6d brd ff:ff:ff:ff:ff:ff
</code></pre>

<p>修改队列长度</p>

<pre><code class="bash"># ifconfig vethd4ea txqueuelen 0
</code></pre>

<p>删除qdisc</p>

<pre><code class="bash"># tc qdisc del dev vethd4ea root                    
# ip link show vethd4ea                
9: vethd4ea: &lt;BROADCAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue master docker0 state UP mode DEFAULT 
link/ether 3a:15:3b:e1:d7:6d brd ff:ff:ff:ff:ff:ff
</code></pre>

<p>可以看到，UP的veth设备成功修改成noqueue。</p>

<h2>小结</h2>

<p>总之，给虚拟网络设备创建默认的qdisc，是不太合理的。这会让虚拟机（或者容器）的网络瓶颈过早的出现在qdisc，而不是真实的物理设备（除非应用需要创建qdisc）。更多详细参考<a href="https://bugzilla.redhat.com/show_bug.cgi?id=1152231">这里</a>。</p>
]]></content>
  </entry>
  
</feed>
